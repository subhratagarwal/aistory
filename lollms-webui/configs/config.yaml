# =================== Lord Of Large Language Multimodal Systems Configuration file =========================== 
version: 161

# topbar
current_language: english

# skills library
activate_skills_lib: false # Activate vectorizing previous conversations
skills_lib_database_name: "default" # Default skills database

max_summary_size: 512 # in tokens

# video viewing and news recovering
last_viewed_video: null
last_viewed_changelog_version: null

binding_name: null
model_name: null
model_variant: null
model_type: null

show_news_panel: false

# Security measures
turn_on_setting_update_validation: true
turn_on_code_execution: true
turn_on_code_validation: true
turn_on_open_file_validation: true
turn_on_send_file_validation: true
turn_on_language_validation: true

force_accept_remote_access: false

# Execution engines parameters
pdf_latex_path: null

# Server information
headless_server_mode: false
allowed_origins: []

# Host information
host: localhost
port: 9600

app_custom_logo: ""
app_custom_name: ""
app_show_changelogs: true
app_custom_welcome_message: ""
app_custom_slogan: ""
app_show_fun_facts: true

# Genreration parameters 
discussion_prompt_separator: "!@>"
start_header_id_template: "!@>"
end_header_id_template: ": "

separator_template: "\n"

start_user_header_id_template: "!@>"
end_user_header_id_template: ": "
end_user_message_id_template: ""

start_ai_header_id_template: "!@>"
end_ai_header_id_template: ": "
end_ai_message_id_template: ""

system_message_template: "system"
use_continue_message: true

seed: -1
ctx_size: 4084
max_n_predict: 4084
min_n_predict: 1024
temperature: 0.9
top_k: 50
top_p: 0.95
repeat_last_n: 40
repeat_penalty: 1.2
num_experts_per_token: 2

n_threads: 8

#Personality parameters
personalities: ["generic/lollms"]
active_personality_id: 0
override_personality_model_parameters: false #if true the personality parameters are overriden by those of the configuration (may affect personality behaviour) 


# interaction parameters
user_name: user
user_description: ""
use_assistant_name_in_discussion: true
use_user_name_in_discussions: true
use_model_name_in_discussions: false
user_avatar: null
use_user_informations_in_discussion: false

# UI parameters
discussion_db_name: default

#automatic stuff
auto_show_browser: true
auto_save: true
auto_title: false

# debug information
debug: false
debug_show_final_full_prompt: false
debug_show_chunks: false
debug_log_file_path: ""

# Automatic updates
auto_update: true
auto_sync_personalities: true
auto_sync_extensions: true
auto_sync_bindings: true
auto_sync_models: true

# lollms service
enable_lollms_service: false
lollms_access_keys : [] # set a list of keys separated by coma to restrict access
activate_lollms_server: true
activate_lollms_rag_server: true
activate_lollms_tts_server: true
activate_lollms_stt_server: true
activate_lollms_tti_server: true
activate_lollms_itt_server: true
activate_lollms_ttm_server: true
activate_ollama_emulator: true
activate_openai_emulator: true
activate_mistralai_emulator: true


# Install mode (cpu, cpu-noavx, nvidia-tensorcores, nvidia, amd-noavx, amd, apple-intel, apple-silicon)
hardware_mode: nvidia-tensorcores

# copy to clipboard 
copy_to_clipboard_add_all_details: false

# -------------------- Services global configurations --------------------------
# Select the active test to speach, text to image and speach to text services
active_tts_service: "None" # xtts (offline), openai_tts (API key required), elevenlabs_tts, fish_tts (API key required)
active_tti_service: "None" # autosd (offline), diffusers (offline), diffusers_client (online), dall-e (online), midjourney (online)
active_stt_service: "None" # whisper (offline), asr (offline or online), openai_whiosper (API key required)
active_ttm_service: "None" # musicgen (offline)
active_ttv_service: "None" # novita_ai, cog_video_x, diffusers, lumalab (offline)
# -------------------- Services --------------------------

# ***************** STT *****************
stt_input_device: 0
stt_listening_threshold: 1000
stt_silence_duration: 2
stt_sound_threshold_percentage: 10
stt_gain: 1.0 
stt_rate: 44100
stt_channels: 1
stt_buffer_size: 10

stt_activate_word_detection: false
stt_word_detection_file: null


# ***************** TTS *****************
tts_output_device: 0
auto_read: false

# ***************** TTV *****************

# ***************** TTT *****************


# Smart router
use_smart_routing: false
smart_routing_router_model : ""
smart_routing_models_description : {}
restore_model_after_smart_routing : false


# Audio
media_on: false
audio_in_language: 'en-US'
auto_speak: false
audio_out_voice: null
audio_pitch: 1
audio_auto_send_input: true
audio_silenceTimer: 5000

# relmote databases
# This is the list of datalakes to be used for RAG
# Datalakes hae the following entries
#
datalakes: [] 

# Data vectorization
rag_local_services: [] # This is the list of rag services served locally 

rag_vectorizer: semantic # possible values semantic, tfidf, openai, ollama
rag_service_url: "http://localhost:11434" # rag service url for ollama
rag_vectorizer_model:  "BAAI/bge-m3" # The model name if applicable
rag_vectorizer_execute_remote_code:  false # do not execute remote code or do
rag_vectorizer_parameters: null # Parameters of the model in json format
rag_chunk_size: 512 # number of tokens per chunk
rag_overlap: 0 # number of tokens of overlap
rag_min_correspondance: 0 # minimum correspondance between the query and the content

rag_n_chunks: 4 #Number of chunks to recover from the database
rag_clean_chunks: true #Removed all uinecessary spaces and line returns
rag_follow_subfolders: true #if true the vectorizer will vectorize the content of subfolders too
rag_check_new_files_at_startup: false #if true, the vectorizer will automatically check for any new files in the folder and adds it to the database
rag_preprocess_chunks: false #if true, an LLM will preprocess the content of the chunk before writing it in a simple format
rag_activate_multi_hops: false #if true, we use multi hops algorithm to do multiple researches until the AI has enough data
rag_min_nb_tokens_in_chunk: 10 #this removed any useless junk ith less than x tokens
rag_max_n_hops: 3 #We set the maximum number of hop in multi hops rag

rag_deactivate: false # if you have a large context model, you can activate this to use your document as a whole
rag_vectorizer_openai_key: "" # The open ai key (if not provided, this will use the environment varaible OPENAI_API_KEY)

contextual_summary: false #If activated this will completely replace the rag and instead will use contextual summary

rag_put_chunk_informations_into_context: false # if true then each chunk will be preceded by its information which may waste some context space but allow the ai to point where it found th einformation
rag_build_keys_words: true # If true, when querrying the database, we use keywords generated from the user prompt instead of the prompt itself.

# Activate internet search
activate_internet_search: false
activate_internet_pages_judgement: true
internet_vectorization_chunk_size: 512 # chunk size
internet_vectorization_overlap_size: 0 # overlap between chunks size
internet_vectorization_nb_chunks: 4 # number of chunks to use
internet_nb_search_pages: 8 # number of pages to select
internet_quick_search: false # If active the search engine will not load and read the webpages
internet_activate_search_decision: false # If active the ai decides by itself if it needs to do search


# boosting information
positive_boost: null
negative_boost: null
fun_mode: false
think_first_mode: false
thinking_prompt:   "Use a think first process to answer the user:
  <think>
  Ask yourself about the user's request and answer it with logical details.
  If the user is requesting general information that does not require internet search and you are confident about it, then prepare to answer directly.
  If the user is requesting general information that does require internet search and you have in the context enough information to answer, then use that data to answer.
  If the user is requesting general information that does require internet search but you do not have any information, then ask him to activate internet search.

  if the user is posing a riddle or asking a math question, make sure you use regourous math hypothisis, testing and analysis.
  If the user is requesting to perform a task, then plan it through steps and prepare to answer
  If the user is just discussing casually, do not perform the think first process

  Make sure you continue thinking until you find a satisfactory answer
  Assess any potential errors you may make
  </think>

  After thinking you can answer the user."




mounted_function_calls: []
# { name: the function name,
#   author: the author of the function
#   category: the category of the function
#   value: the function name without spaces,
#   selected: selected or not,
#   icon: the icon in form feather:icon name or img:url or b64:base64,
#   help: the help
# }

# webui configurations
show_code_of_conduct: true
activate_audio_infos: true

keep_thoughts: false

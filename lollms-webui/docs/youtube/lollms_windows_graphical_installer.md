Hi there! Are you ready to discover the power of lollms? In this video, we'll take a step-by-step journey through installing the lollms system from my GitHub repository. Let's get started!

First, open your favorite web browser and type "lollms-webui" into the search bar. Click on the first link that appears, which will take you to the GitHub repository. Look for the latest release link on the page, and you'll find multiple options for installing lollms on Windows or Mac. For this video, we'll focus on the executable installer for Windows.

Once you've downloaded the installer, Windows may display a warning about not trusting it. Don't worry; this is because the installer is new and not yet recognized by the system. Click "Run anyway" to proceed with the installation.

The installation process is straightforward. First, select your preferred language and accept the license agreement. Please take a moment to read the code of conduct, which outlines important moral guidelines for using lollms. By installing lollms, you agree to respect these conditions, which include not promoting violence, misinformation, or any behavior that can harm living creatures or destroy infrastructure.

Next, choose the installation folder and decide whether you want a shortcut on your desktop. Then, select the binding you want to use. In this video, we'll be using ollama.

Make sure to keep the "Execute first_install" checkbox checked, as this will run the installation script. The installer will set up a portable version of Conda with Python 3.11, clone the repository code, install dependencies, and prepare all the necessary files and shortcuts.

If you selected ollama as your binding, you'll be prompted to install it. If you already have it installed, you can skip this step. Otherwise, go ahead and install ollama.

Once the installation is complete, lollms will automatically load if the "Run lollms" checkbox was checked. If it was unchecked, simply run lollms from the desktop or the Windows Start menu.

Lollms has a Python backend and a Vue.js frontend served via FastAPI. The backend uses both FastAPI and Socket.IO to enable connected and disconnected communication, making it usable as a service for compatible clients that can use the lollms_client library, lollms_js library, or the Unreal Engine plugin.

Notice that the welcome page will popup automatically which links to the official lollms website. This page can be deactivated easily if you don't want to have it show up at startup. You can always load it using the information button on the top right of the page.

Before generating content, you need to download and select a model. In this video, we'll choose llava, a multimodal model that can understand both text and images.

Now, navigate to the discussions view and start chatting with your AI. Let's ask it what time it is and what you can do today. Notice the decent generation speed of over 55 tokens per second.

Let's take it a step further and ask the AI to write a poem about itself. Then, use the Melody Maestro persona to transform the poem into a song. Once you have the song, open Suno AI and generate music using the guidelines provided by Melody Maestro. Listen to the generated song and appreciate the AI's creativity!

For an extra twist, let's make the song heavy metal. The result is a catchy and cool tune that showcases the power of lollms and AI-generated content.

To uninstall lollms, start by turning it off by closing the console window, then turn off ollama.
First uninstall ollama, then uninstall lollms from your application management tool on windows.

There you have it, a full tutorial on install, basic usage and uninstall the tool.

Thank you for joining me on this exciting journey through lollms installation and content generation. Stay tuned for more amazing videos exploring the world of AI and robotics!

See ya!